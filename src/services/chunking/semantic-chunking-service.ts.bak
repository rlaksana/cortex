/**
 * Semantic Chunking Service - High-Performance Document Processing
 *
 * Provides intelligent document chunking with configurable parameters,
 * semantic boundary detection, and performance optimization.
 *
 * Performance Targets:
 * - <100ms for 1MB documents
 * - Configurable chunk sizes and overlap
 * - Multiple chunking strategies
 * - Memory-efficient processing
 *
 * @author Cortex Team
 * @version 2.0.1
 * @since 2025
 */

import { logger } from '../../utils/logger.js';
import { PerformanceMonitor } from '../core/performance-monitor.js';

/**
 * Chunking strategy options
 */
export type ChunkingStrategy =
  | 'semantic' // Intelligent semantic boundary detection
  | 'fixed_size' // Fixed size chunks with overlap
  | 'paragraph' // Paragraph-based chunking
  | 'sentence' // Sentence-based chunking
  | 'token' // Token-based chunking
  | 'hybrid'; // Combines multiple strategies

/**
 * Chunking configuration
 */
export interface ChunkingConfig {
  /** Chunking strategy to use */
  strategy: ChunkingStrategy;

  /** Target chunk size in characters */
  chunkSize: number;

  /** Overlap between chunks in characters */
  overlap: number;

  /** Minimum chunk size in characters */
  minChunkSize: number;

  /** Maximum chunk size in characters */
  maxChunkSize: number;

  /** Whether to preserve sentence boundaries */
  preserveSentenceBoundaries: boolean;

  /** Whether to preserve paragraph boundaries */
  preserveParagraphBoundaries: boolean;

  /** Custom separators for chunking */
  separators?: string[];

  /** Language-specific settings */
  language?: 'en' | 'zh' | 'ja' | 'ko' | 'auto';

  /** Performance optimization settings */
  performance?: {
    /** Enable parallel processing for large documents */
    enableParallel: boolean;
    /** Batch size for parallel processing */
    batchSize: number;
    /** Maximum memory usage in MB */
    maxMemoryMB: number;
  };
}

/**
 * Document chunk with metadata
 */
export interface DocumentChunk {
  /** Chunk identifier */
  id: string;

  /** Chunk content */
  content: string;

  /** Chunk position in document */
  position: {
    /** Start character index */
    start: number;
    /** End character index */
    end: number;
    /** Chunk index (0-based) */
    index: number;
  };

  /** Chunk metadata */
  metadata: {
    /** Chunk size in characters */
    size: number;
    /** Chunk size in tokens (approximate) */
    tokenCount: number;
    /** Whether chunk ends at sentence boundary */
    endsAtSentenceBoundary: boolean;
    /** Whether chunk ends at paragraph boundary */
    endsAtParagraphBoundary: boolean;
    /** Semantic relevance score (0-1) */
    semanticScore?: number;
    /** Chunk strategy used */
    strategy: ChunkingStrategy;
  };

  /** Timing information */
  timing?: {
    /** Time to process chunk in ms */
    processingTime: number;
    /** Memory usage in bytes */
    memoryUsage: number;
  };
}

/**
 * Chunking result with performance metrics
 */
export interface ChunkingResult {
  /** Array of document chunks */
  chunks: DocumentChunk[];

  /** Processing statistics */
  statistics: {
    /** Total document size in characters */
    documentSize: number;
    /** Total chunks created */
    totalChunks: number;
    /** Average chunk size */
    averageChunkSize: number;
    /** Smallest chunk size */
    minChunkSize: number;
    /** Largest chunk size */
    maxChunkSize: number;
    /** Total overlap characters */
    totalOverlap: number;
  };

  /** Performance metrics */
  performance: {
    /** Total processing time in ms */
    totalProcessingTime: number;
    /** Average processing time per chunk in ms */
    averageChunkTime: number;
    /** Peak memory usage in MB */
    peakMemoryUsage: number;
    /** Chunks per second processing rate */
    processingRate: number;
  };

  /** Configuration used */
  config: ChunkingConfig;
}

/**
 * Default chunking configuration
 */
export const DEFAULT_CHUNKING_CONFIG: ChunkingConfig = {
  strategy: 'semantic',
  chunkSize: 1000,
  overlap: 100,
  minChunkSize: 200,
  maxChunkSize: 2000,
  preserveSentenceBoundaries: true,
  preserveParagraphBoundaries: true,
  language: 'auto',
  performance: {
    enableParallel: true,
    batchSize: 5,
    maxMemoryMB: 512,
  },
};

/**
 * Semantic Chunking Service
 */
export class SemanticChunkingService {
  private static instance: SemanticChunkingService;
  private performanceMonitor: PerformanceMonitor;
  private isInitialized = false;

  private constructor() {
    this.performanceMonitor = PerformanceMonitor.getInstance();
  }

  public static getInstance(): SemanticChunkingService {
    if (!SemanticChunkingService.instance) {
      SemanticChunkingService.instance = new SemanticChunkingService();
    }
    return SemanticChunkingService.instance;
  }

  /**
   * Initialize the chunking service
   */
  public async initialize(): Promise<void> {
    if (this.isInitialized) {
      return;
    }

    try {
      logger.info('Initializing Semantic Chunking Service...');

      // Initialize any required resources
      await this.initializeLanguageModels();

      this.isInitialized = true;
      logger.info('Semantic Chunking Service initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Semantic Chunking Service:', error);
      throw error;
    }
  }

  /**
   * Chunk a document using the specified configuration
   */
  public async chunkDocument(
    document: string,
    config: Partial<ChunkingConfig> = {}
  ): Promise<ChunkingResult> {
    const startTime = Date.now();
    const startMemory = process.memoryUsage().heapUsed;

    try {
      const finalConfig = { ...DEFAULT_CHUNKING_CONFIG, ...config };

      logger.debug(
        `Chunking document (${String(document?.length ?? 0)} chars) with strategy: ${finalConfig.strategy}`
      );

      // Validate input
      if (!document || typeof document !== 'string') {
        throw new Error('Document must be a non-empty string');
      }

      // Select chunking strategy
      const chunks = await this.executeChunkingStrategy(document, finalConfig);

      // Calculate statistics
      const statistics = this.calculateStatistics(document, chunks);

      // Calculate performance metrics
      const endTime = Date.now();
      const endMemory = process.memoryUsage().heapUsed;
      const performance = {
        totalProcessingTime: endTime - startTime,
        averageChunkTime: (endTime - startTime) / chunks.length,
        peakMemoryUsage: (endMemory - startMemory) / (1024 * 1024), // MB
        processingRate: (chunks.length / (endTime - startTime)) * 1000, // chunks per second
      };

      // Check performance targets
      if (performance.totalProcessingTime > 100) {
        logger.warn(
          `Chunking performance target exceeded: ${performance.totalProcessingTime}ms > 100ms`
        );
      }

      const result: ChunkingResult = {
        chunks,
        statistics,
        performance,
        config: finalConfig,
      };

      // Record performance metrics
      this.performanceMonitor.recordMetrics({
        timestamp: new Date(),
        service: 'semantic-chunking',
        operation: 'chunk',
        duration: performance.totalProcessingTime,
        success: true,
        memoryUsage: {
          heapUsed: performance.peakMemoryUsage,
          heapTotal: performance.peakMemoryUsage,
          external: 0,
          arrayBuffers: 0,
          rss: performance.peakMemoryUsage,
        },
      });

      logger.info(
        `Document chunked into ${String(chunks?.length ?? 0)} chunks in ${performance.totalProcessingTime}ms`
      );
      return result;
    } catch (error) {
      logger.error('Document chunking failed:', error);
      throw error;
    }
  }

  /**
   * Chunk multiple documents in parallel
   */
  public async chunkDocuments(
    documents: string[],
    config: Partial<ChunkingConfig> = {}
  ): Promise<ChunkingResult[]> {
    const finalConfig = { ...DEFAULT_CHUNKING_CONFIG, ...config };

    if (!finalConfig.performance?.enableParallel || documents.length <= 1) {
      // Process sequentially
      const _results: ChunkingResult[] = [];
      for (const document of documents) {
        results.push(await this.chunkDocument(document, config));
      }
      return results;
    }

    // Process in parallel batches
    const batchSize = finalConfig.performance.batchSize || 5;
    const _results: ChunkingResult[] = [];

    for (let i = 0; i < documents.length; i += batchSize) {
      const batch = documents.slice(i, i + batchSize);
      const batchPromises = batch.map((doc) => this.chunkDocument(doc, config));
      const batchResults = await Promise.all(batchPromises);
      results.push(...batchResults);
    }

    return results;
  }

  /**
   * Get supported chunking strategies
   */
  public getSupportedStrategies(): ChunkingStrategy[] {
    return ['semantic', 'fixed_size', 'paragraph', 'sentence', 'token', 'hybrid'];
  }

  /**
   * Validate chunking configuration
   */
  public validateConfig(config: Partial<ChunkingConfig>): { valid: boolean; errors: string[] } {
    const errors: string[] = [];

    if (config.chunkSize !== undefined) {
      if (config.chunkSize <= 0) {
        errors.push('chunkSize must be positive');
      }
      if (config.chunkSize > 100000) {
        errors.push('chunkSize must be less than 100,000 characters');
      }
    }

    if (config.overlap !== undefined) {
      if (config.overlap < 0) {
        errors.push('overlap cannot be negative');
      }
      if (config.chunkSize && config.overlap >= config.chunkSize) {
        errors.push('overlap must be less than chunkSize');
      }
    }

    if (config.minChunkSize !== undefined && config.maxChunkSize !== undefined) {
      if (config.minChunkSize >= config.maxChunkSize) {
        errors.push('minChunkSize must be less than maxChunkSize');
      }
    }

    if (config.strategy && !this.getSupportedStrategies().includes(config.strategy)) {
      errors.push(`Unsupported strategy: ${config.strategy}`);
    }

    return {
      valid: errors.length === 0,
      errors,
    };
  }

  // Private methods

  private async initializeLanguageModels(): Promise<void> {
    // Initialize any language detection or semantic analysis models
    // For now, we'll use rule-based approaches
    logger.debug('Language models initialized (rule-based)');
  }

  private async executeChunkingStrategy(
    document: string,
    config: ChunkingConfig
  ): Promise<DocumentChunk[]> {
    switch (config.strategy) {
      case 'semantic':
        return await this.chunkSemantic(document, config);
      case 'fixed_size':
        return this.chunkFixedSize(document, config);
      case 'paragraph':
        return this.chunkParagraph(document, config);
      case 'sentence':
        return this.chunkSentence(document, config);
      case 'token':
        return this.chunkToken(document, config);
      case 'hybrid':
        return await this.chunkHybrid(document, config);
      default:
        throw new Error(`Unsupported chunking strategy: ${config.strategy}`);
    }
  }

  private async chunkSemantic(document: string, config: ChunkingConfig): Promise<DocumentChunk[]> {
    const chunks: DocumentChunk[] = [];
    const sentences = this.splitSentences(document);
    const paragraphs = this.splitParagraphs(document);

    let currentIndex = 0;
    let chunkIndex = 0;

    while (currentIndex < document.length) {
      let chunkEnd = Math.min(currentIndex + config.chunkSize, document.length);
      let chunkContent = document.substring(currentIndex, chunkEnd);

      // Try to extend to semantic boundaries
      if (config.preserveSentenceBoundaries) {
        chunkEnd = this.findNearestSentenceBoundary(document, chunkEnd, config.maxChunkSize);
        chunkContent = document.substring(currentIndex, chunkEnd);
      }

      if (config.preserveParagraphBoundaries) {
        chunkEnd = this.findNearestParagraphBoundary(document, chunkEnd, config.maxChunkSize);
        chunkContent = document.substring(currentIndex, chunkEnd);
      }

      // Ensure minimum chunk size
      if (chunkContent.length < config.minChunkSize && chunkEnd < document.length) {
        chunkEnd = Math.min(currentIndex + config.minChunkSize, document.length);
        chunkContent = document.substring(currentIndex, chunkEnd);
      }

      const chunk: DocumentChunk = {
        id: `chunk_${chunkIndex}_${Date.now()}`,
        content: chunkContent,
        position: {
          start: currentIndex,
          end: chunkEnd,
          index: chunkIndex,
        },
        metadata: {
          size: chunkContent.length,
          tokenCount: this.estimateTokenCount(chunkContent),
          endsAtSentenceBoundary: this.endsWithSentenceBoundary(chunkContent),
          endsAtParagraphBoundary: this.endsWithParagraphBoundary(chunkContent),
          semanticScore: await this.calculateSemanticScore(chunkContent),
          strategy: 'semantic',
        },
        timing: {
          processingTime: 0, // Will be set later
          memoryUsage: 0, // Will be set later
        },
      };

      chunks.push(chunk);

      // Calculate next position with overlap
      currentIndex = Math.max(currentIndex + 1, chunkEnd - config.overlap);
      chunkIndex++;
    }

    return chunks;
  }

  private chunkFixedSize(_document: string,  config: ChunkingConfig): DocumentChunk[] {
    const chunks: DocumentChunk[] = [];
    let chunkIndex = 0;

    for (let start = 0; start < document.length; start += config.chunkSize - config.overlap) {
      const end = Math.min(start + config.chunkSize, document.length);
      const content = document.substring(start, end);

      if (content.length >= config.minChunkSize) {
        chunks.push({
          id: `chunk_${chunkIndex}_${Date.now()}`,
          content,
          position: { start, end, index: chunkIndex },
          metadata: {
            size: content.length,
            tokenCount: this.estimateTokenCount(content),
            endsAtSentenceBoundary: this.endsWithSentenceBoundary(content),
            endsAtParagraphBoundary: this.endsWithParagraphBoundary(content),
            strategy: 'fixed_size',
          },
        });
        chunkIndex++;
      }
    }

    return chunks;
  }

  private chunkParagraph(_document: string,  config: ChunkingConfig): DocumentChunk[] {
    const paragraphs = this.splitParagraphs(document);
    const chunks: DocumentChunk[] = [];
    let currentChunk = '';
    let currentIndex = 0;
    let chunkIndex = 0;

    for (const paragraph of paragraphs) {
      const testChunk = currentChunk + (currentChunk ? '\n\n' : '') + paragraph;

      if (testChunk.length <= config.chunkSize) {
        currentChunk = testChunk;
      } else {
        if (currentChunk.length >= config.minChunkSize) {
          chunks.push(
            this.createChunk(
              currentChunk,
              currentIndex,
              currentIndex + currentChunk.length,
              chunkIndex,
              'paragraph'
            )
          );
          currentIndex += currentChunk.length;
          chunkIndex++;
        }
        currentChunk = paragraph;
      }
    }

    // Add final chunk
    if (currentChunk.length >= config.minChunkSize) {
      chunks.push(
        this.createChunk(
          currentChunk,
          currentIndex,
          currentIndex + currentChunk.length,
          chunkIndex,
          'paragraph'
        )
      );
    }

    return chunks;
  }

  private chunkSentence(_document: string,  config: ChunkingConfig): DocumentChunk[] {
    const sentences = this.splitSentences(document);
    const chunks: DocumentChunk[] = [];
    let currentChunk = '';
    let currentIndex = 0;
    let chunkIndex = 0;

    for (const sentence of sentences) {
      const testChunk = currentChunk + (currentChunk ? ' ' : '') + sentence;

      if (testChunk.length <= config.chunkSize) {
        currentChunk = testChunk;
      } else {
        if (currentChunk.length >= config.minChunkSize) {
          chunks.push(
            this.createChunk(
              currentChunk,
              currentIndex,
              currentIndex + currentChunk.length,
              chunkIndex,
              'sentence'
            )
          );
          currentIndex += currentChunk.length;
          chunkIndex++;
        }
        currentChunk = sentence;
      }
    }

    // Add final chunk
    if (currentChunk.length >= config.minChunkSize) {
      chunks.push(
        this.createChunk(
          currentChunk,
          currentIndex,
          currentIndex + currentChunk.length,
          chunkIndex,
          'sentence'
        )
      );
    }

    return chunks;
  }

  private chunkToken(_document: string,  config: ChunkingConfig): DocumentChunk[] {
    // Approximate token-based chunking (4 chars per token average)
    const targetTokens = Math.floor(config.chunkSize / 4);
    const chunks: DocumentChunk[] = [];
    let chunkIndex = 0;

    const words = document.split(/\s+/);
    let currentChunk = '';
    let currentTokens = 0;
    let currentIndex = 0;

    for (const word of words) {
      const wordTokens = Math.ceil(word.length / 4);

      if (currentTokens + wordTokens <= targetTokens) {
        currentChunk += (currentChunk ? ' ' : '') + word;
        currentTokens += wordTokens;
      } else {
        if (currentChunk.length >= config.minChunkSize) {
          chunks.push(
            this.createChunk(
              currentChunk,
              currentIndex,
              currentIndex + currentChunk.length,
              chunkIndex,
              'token'
            )
          );
          currentIndex += currentChunk.length;
          chunkIndex++;
        }
        currentChunk = word;
        currentTokens = wordTokens;
      }
    }

    // Add final chunk
    if (currentChunk.length >= config.minChunkSize) {
      chunks.push(
        this.createChunk(
          currentChunk,
          currentIndex,
          currentIndex + currentChunk.length,
          chunkIndex,
          'token'
        )
      );
    }

    return chunks;
  }

  private async chunkHybrid(document: string, config: ChunkingConfig): Promise<DocumentChunk[]> {
    // Hybrid strategy: try semantic first, fall back to paragraph if needed
    try {
      const semanticChunks = await this.chunkSemantic(document, config);

      // Check if semantic chunking produced good results
      const avgSemanticScore =
        semanticChunks.reduce((sum,  _chunk) => sum + (chunk.metadata.semanticScore || 0), 0) /
        semanticChunks.length;

      if (avgSemanticScore > 0.7) {
        return semanticChunks;
      }
    } catch (error) {
      logger.warn('Semantic chunking failed, falling back to paragraph strategy:', error);
    }

    // Fall back to paragraph chunking
    return this.chunkParagraph(document, config);
  }

  private createChunk(
    _content: string, 
    _start: number, 
    _end: number, 
    _index: number, 
    _strategy: ChunkingStrategy
  ): DocumentChunk {
    return {
      id: `chunk_${index}_${Date.now()}`,
      content,
      position: { start, end, index },
      metadata: {
        size: content.length,
        tokenCount: this.estimateTokenCount(content),
        endsAtSentenceBoundary: this.endsWithSentenceBoundary(content),
        endsAtParagraphBoundary: this.endsWithParagraphBoundary(content),
        strategy,
      },
    };
  }

  private splitSentences(_text: string): string[] {
    // Enhanced sentence splitting for multiple languages
    return text.split(/(?<=[.!?。！？])\s+/).filter((s) => s.trim().length > 0);
  }

  private splitParagraphs(_text: string): string[] {
    return text.split(/\n\s*\n/).filter((p) => p.trim().length > 0);
  }

  private findNearestSentenceBoundary(_text: string,  _position: number,  _maxPosition: number): number {
    const sentenceEndings = /[.!?。！？]/g;
    let match;

    // Search forward from current position
    sentenceEndings.lastIndex = position;
    while ((match = sentenceEndings.exec(text)) && match.index < maxPosition) {
      return match.index + 1;
    }

    return Math.min(position, maxPosition);
  }

  private findNearestParagraphBoundary(
    _text: string, 
    _position: number, 
    _maxPosition: number
  ): number {
    const paragraphEndings = /\n\s*\n/g;
    let match;

    // Search forward from current position
    paragraphEndings.lastIndex = position;
    while ((match = paragraphEndings.exec(text)) && match.index < maxPosition) {
      return match.index + 1;
    }

    return Math.min(position, maxPosition);
  }

  private endsWithSentenceBoundary(_text: string): boolean {
    return /[.!?。！？]\s*$/.test(text.trim());
  }

  private endsWithParagraphBoundary(_text: string): boolean {
    return /\n\s*$/.test(text);
  }

  private estimateTokenCount(_text: string): number {
    // Rough estimation: ~4 characters per token for English
    // This varies by language but provides a reasonable approximation
    return Math.ceil(text.length / 4);
  }

  private async calculateSemanticScore(content: string): Promise<number> {
    // Simple semantic scoring based on content characteristics
    // In a real implementation, this could use language models
    let score = 0.5; // Base score

    // Boost for complete sentences
    const sentences = this.splitSentences(content);
    score += Math.min(sentences.length * 0.1, 0.3);

    // Boost for structured content
    if (content.includes('\n')) score += 0.1;
    if (/:/.test(content)) score += 0.05; // Contains lists or definitions

    // Boost for appropriate length
    if (content.length > 100 && content.length < 1000) score += 0.1;

    return Math.min(score, 1.0);
  }

  private calculateStatistics(_document: string,  _chunks: DocumentChunk[]) {
    const sizes = chunks.map((chunk) => chunk.metadata.size);

    return {
      documentSize: document.length,
      totalChunks: chunks.length,
      averageChunkSize: sizes.reduce((sum,  _size) => sum + size, 0) / sizes.length,
      minChunkSize: Math.min(...sizes),
      maxChunkSize: Math.max(...sizes),
      totalOverlap: chunks.reduce((sum,  _chunk,  _index) => {
        if (index === 0) return 0;
        const prevChunk = chunks[index - 1];
        return sum + Math.max(0, prevChunk.position.end - chunk.position.start);
      }, 0),
    };
  }
}

// Export singleton instance
export const semanticChunkingService = SemanticChunkingService.getInstance();
